# Eilson He
#CS 360 Final Final Project

"""
A model simulating how an instant particular pollution will spread between cities.
This algorithm assumes that pollution remains constant in one period, i.e.
the sum of all pollution from one source is equal to the decrease of pollution
in the same city. This description might not be realistic, but it will help us
analyze the spreading pattern in a particular network.
"""

import networkx as nx
import random
import copy
import numpy
from statistics import mean
import matplotlib.pyplot as plt

#generate a random weighted directed graph
def np_graph(n, p):
    G = nx.Graph()
    #i['pollution'] records the current pollution in city i. i['phi'] represents the new pollution generated by city i per turn.
    for i in range(n):
        G.add_node(i, pollution = 0)
    for x in range(n):
        for y in range(x + 1, n):
            if random.random() < p:
                G.add_edge(x, y)
    print('NP_Graph generated')
    return G

def price_model(n, n0, c, r):
    G = nx.Graph()
    #i['pollution'] records the current pollution in city i. i['phi'] represents the new pollution generated by city i per turn.
    for i in range(n):
        G.add_node(i, pollution = 0)
    nodes = [node for node in range(n0) for _ in range(c + r)]
    for n1 in range(n0, n):
        targets = set()
        while len(targets) < c:
            x = random.choice(nodes)
            targets.add(x)
        for n2 in targets:
            G.add_edge(n1, n2)
        nodes += targets
        for _ in range(r):
            nodes.append(n1)
    print('Price_model generated')
    return G

def spread(G, alpha):
    """
    G: network
    alpha: a modifying factor depending on the specific pollution
    return the pollution distribution at the end of the turn.
    """
    temp_poll = [G.nodes[i]['pollution'] for i in range(G.number_of_nodes())]
    nodes = list(G.nodes)
    for curr_node in nodes:
        for dest in G.neighbors(curr_node):
            transfer = min(G.nodes[curr_node]['pollution'], alpha * (G.nodes[curr_node]['pollution'] - G.nodes[dest]['pollution']) * G.nodes[curr_node]['pollution'])
            temp_poll[curr_node] -= transfer
            temp_poll[dest] += transfer
    return temp_poll

def distance_k(G, source):
    """
    G: network
    source: source node
    return a dict of all nodes with distance l away from the source
    """
    n = G.number_of_nodes()
    d = [-1 for _ in range(n)]
    d[source] = 0
    dis = {0: [source]}
    queue = [source]
    while queue:
        lst = dis.keys()
        s = queue.pop(0)
        for node in G.neighbors(s):
            if d[node] == -1:
                d[node] = d[s] + 1
                if d[node] not in lst:
                    dis[d[node]] = []
                dis[d[node]].append(node)
                queue.append(node)
    return dis

def modularity_maximization(G):
    """
    G: network
    return: dictionary comm_to_nodes
    """
    n = G.number_of_nodes()
    m = G.number_of_edges()

    # initialization
    G_comms = nx.Graph()
    G_comms.add_nodes_from(range(n))
    for i,j in G.edges():
        G_comms.add_edge(i, j, w=1/m)
    comm_to_stubs = { i: G.degree(i)/(2*m) for i in G.nodes() }
    comm_to_nodes = { i:[i] for i in G.nodes()}

    while True:

        # find two communities to merge
        best_delta = 0
        best_comms = (-1,-1)
        for a,b in G_comms.edges():
            e_ab = G_comms[a][b]['w']
            s_a = comm_to_stubs[a]
            s_b = comm_to_stubs[b]
            delta = e_ab - 2*s_a*s_b
            if delta > best_delta:
                best_delta = delta
                best_comms = (a,b)

        if best_delta == 0:
            # can't improve modularity anymore
            break

        # merge the two communities that give best increase in Q
        a,b = best_comms
        for c in G_comms.neighbors(a):
            if c == b:
                continue
            if G_comms.has_edge(b,c):
                G_comms[b][c]['w'] += G_comms[a][c]['w']
            else:
                G_comms.add_edge(b, c, w=G_comms[a][c]['w'])

        comm_to_stubs[b] += comm_to_stubs[a]
        comm_to_nodes[b] = comm_to_nodes[a] + comm_to_nodes[b]

        del comm_to_stubs[a]
        del comm_to_nodes[a]
        G_comms.remove_node(a)

    return comm_to_nodes

alpha = 1 / 10000

n = 100
c, r = (n - 1) // 18, n // 100
n0 = n // 10
p = 2 * c * (n - n0) / (n * (n - 1))     #set NP and Price model with equal edges
s = n
l = ['betweenness']
np = np_graph(n, p)
pm = price_model(n, n0, c, r)
graphs = [np, pm]
for graph in graphs:
    print('Finding communities')

    #community detection
    comm_class = list(modularity_maximization(np).values())
    print('Communities found')
    for method in l:
        name = 'NP graph.' if graph == np else 'Price model.'
        print('Running for ' + method + ' centrality in ' + str(graph) + name)
        G = copy.deepcopy(graph)
        t = 0

        #get the list sorted in the order of the given centrality
        map = eval('nx.%s_centrality(G)'%method)
        source = max(map, key = map.get)
        G.nodes[source]['pollution'] = s
        distances = distance_k(G, source)
        max_dis = len(distances.keys())

        #pollution in different communities
        comm_poll = [[] for _ in range(len(comm_class))]
        #average pollution of nodes with distance i
        pollutions = [[] for _ in range(max_dis)]
        done = False
        while not done:
            final_poll = spread(G, alpha)
            done = True
            #test if we should stop spreading as little pollution trasfers between nodes.
            for i in range(n):
                done = done and abs(G.nodes[i]['pollution'] - final_poll[i]) <= s / 10000
            for i in range(max_dis):
                pollutions[i].append(mean([G.nodes[j]['pollution'] for j in distances[i]]))
            for comm in range(len(comm_class)):
                comm_poll[comm].append(mean([G.nodes[i]['pollution'] for i in comm_class[comm]]))
            #update new pollutions
            for node in G.nodes:
                G.nodes[node]['pollution'] = final_poll[node]
            t += 1

            if t % 100 == 0:
                print('Running for the ' + str(t) + 'th time.')
        print('Completed.')

        times = [i for i in range(t)]
        #print('Max pollution happens in turn ' + str(pollution_target.index(max(pollution_target))))

        fig1, ax1 = plt.subplots()
        for i in range(max_dis):
            ax1.plot(times, pollutions[i], linewidth = 2, label = 'Nodes with distance ' + str(i))
        ax1.set_title('Average pollution vs. Distance')
        ax1.set_xlabel('t')
        ax1.set_ylabel('pollution')
        ax1.set_ylim(bottom = 0, top = max(pollutions[1]) + 0.5)
        fig2, ax2 = plt.subplots()
        for comm in range(len(comm_poll)):
            ax2.plot(times, comm_poll[comm], linewidth = 2, label = 'Community ' + str(comm) + ' in ' + method + ' centrality measure in ' + name)
        ax2.set_title('Pollution in communities')
        ax2.set_xlabel('t')
        ax2.set_ylabel('pollution')

    ax1.legend()
    fig1.savefig('centrality_' + name + 'pdf')
    ax2.legend()
    fig2.savefig('communities_' + name + 'pdf')
